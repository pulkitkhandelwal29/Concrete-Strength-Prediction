{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concrete Strength Prediction\n",
    "\n",
    "Concrete is the most important material in civil engineering.<br><br><br>\n",
    "**We will be predicting strength of Concrete based on certain features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading dataset\n",
    "df = pd.read_csv('Concrete_Data_Yeh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
       "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
       "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
       "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
       "\n",
       "   fineaggregate  age  csMPa  \n",
       "0          676.0   28  79.99  \n",
       "1          676.0   28  61.89  \n",
       "2          594.0  270  40.27  \n",
       "3          594.0  365  41.05  \n",
       "4          825.5  360  44.30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1030 entries, 0 to 1029\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   cement            1030 non-null   float64\n",
      " 1   slag              1030 non-null   float64\n",
      " 2   flyash            1030 non-null   float64\n",
      " 3   water             1030 non-null   float64\n",
      " 4   superplasticizer  1030 non-null   float64\n",
      " 5   coarseaggregate   1030 non-null   float64\n",
      " 6   fineaggregate     1030 non-null   float64\n",
      " 7   age               1030 non-null   int64  \n",
      " 8   csMPa             1030 non-null   float64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 72.5 KB\n"
     ]
    }
   ],
   "source": [
    "#Information about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cement         slag       flyash        water  superplasticizer  \\\n",
       "count  1030.000000  1030.000000  1030.000000  1030.000000       1030.000000   \n",
       "mean    281.167864    73.895825    54.188350   181.567282          6.204660   \n",
       "std     104.506364    86.279342    63.997004    21.354219          5.973841   \n",
       "min     102.000000     0.000000     0.000000   121.800000          0.000000   \n",
       "25%     192.375000     0.000000     0.000000   164.900000          0.000000   \n",
       "50%     272.900000    22.000000     0.000000   185.000000          6.400000   \n",
       "75%     350.000000   142.950000   118.300000   192.000000         10.200000   \n",
       "max     540.000000   359.400000   200.100000   247.000000         32.200000   \n",
       "\n",
       "       coarseaggregate  fineaggregate          age        csMPa  \n",
       "count      1030.000000    1030.000000  1030.000000  1030.000000  \n",
       "mean        972.918932     773.580485    45.662136    35.817961  \n",
       "std          77.753954      80.175980    63.169912    16.705742  \n",
       "min         801.000000     594.000000     1.000000     2.330000  \n",
       "25%         932.000000     730.950000     7.000000    23.710000  \n",
       "50%         968.000000     779.500000    28.000000    34.445000  \n",
       "75%        1029.400000     824.000000    56.000000    46.135000  \n",
       "max        1145.000000     992.600000   365.000000    82.600000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Description of dataset\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement              0\n",
       "slag                0\n",
       "flyash              0\n",
       "water               0\n",
       "superplasticizer    0\n",
       "coarseaggregate     0\n",
       "fineaggregate       0\n",
       "age                 0\n",
       "csMPa               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cement</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.275216</td>\n",
       "      <td>-0.397467</td>\n",
       "      <td>-0.081587</td>\n",
       "      <td>0.092386</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>-0.222718</td>\n",
       "      <td>0.081946</td>\n",
       "      <td>0.497832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slag</th>\n",
       "      <td>-0.275216</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.323580</td>\n",
       "      <td>0.107252</td>\n",
       "      <td>0.043270</td>\n",
       "      <td>-0.283999</td>\n",
       "      <td>-0.281603</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>0.134829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flyash</th>\n",
       "      <td>-0.397467</td>\n",
       "      <td>-0.323580</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.256984</td>\n",
       "      <td>0.377503</td>\n",
       "      <td>-0.009961</td>\n",
       "      <td>0.079108</td>\n",
       "      <td>-0.154371</td>\n",
       "      <td>-0.105755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>-0.081587</td>\n",
       "      <td>0.107252</td>\n",
       "      <td>-0.256984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.657533</td>\n",
       "      <td>-0.182294</td>\n",
       "      <td>-0.450661</td>\n",
       "      <td>0.277618</td>\n",
       "      <td>-0.289633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superplasticizer</th>\n",
       "      <td>0.092386</td>\n",
       "      <td>0.043270</td>\n",
       "      <td>0.377503</td>\n",
       "      <td>-0.657533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.265999</td>\n",
       "      <td>0.222691</td>\n",
       "      <td>-0.192700</td>\n",
       "      <td>0.366079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coarseaggregate</th>\n",
       "      <td>-0.109349</td>\n",
       "      <td>-0.283999</td>\n",
       "      <td>-0.009961</td>\n",
       "      <td>-0.182294</td>\n",
       "      <td>-0.265999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.178481</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>-0.164935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fineaggregate</th>\n",
       "      <td>-0.222718</td>\n",
       "      <td>-0.281603</td>\n",
       "      <td>0.079108</td>\n",
       "      <td>-0.450661</td>\n",
       "      <td>0.222691</td>\n",
       "      <td>-0.178481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.156095</td>\n",
       "      <td>-0.167241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.081946</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>-0.154371</td>\n",
       "      <td>0.277618</td>\n",
       "      <td>-0.192700</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>-0.156095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>csMPa</th>\n",
       "      <td>0.497832</td>\n",
       "      <td>0.134829</td>\n",
       "      <td>-0.105755</td>\n",
       "      <td>-0.289633</td>\n",
       "      <td>0.366079</td>\n",
       "      <td>-0.164935</td>\n",
       "      <td>-0.167241</td>\n",
       "      <td>0.328873</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cement      slag    flyash     water  superplasticizer  \\\n",
       "cement            1.000000 -0.275216 -0.397467 -0.081587          0.092386   \n",
       "slag             -0.275216  1.000000 -0.323580  0.107252          0.043270   \n",
       "flyash           -0.397467 -0.323580  1.000000 -0.256984          0.377503   \n",
       "water            -0.081587  0.107252 -0.256984  1.000000         -0.657533   \n",
       "superplasticizer  0.092386  0.043270  0.377503 -0.657533          1.000000   \n",
       "coarseaggregate  -0.109349 -0.283999 -0.009961 -0.182294         -0.265999   \n",
       "fineaggregate    -0.222718 -0.281603  0.079108 -0.450661          0.222691   \n",
       "age               0.081946 -0.044246 -0.154371  0.277618         -0.192700   \n",
       "csMPa             0.497832  0.134829 -0.105755 -0.289633          0.366079   \n",
       "\n",
       "                  coarseaggregate  fineaggregate       age     csMPa  \n",
       "cement                  -0.109349      -0.222718  0.081946  0.497832  \n",
       "slag                    -0.283999      -0.281603 -0.044246  0.134829  \n",
       "flyash                  -0.009961       0.079108 -0.154371 -0.105755  \n",
       "water                   -0.182294      -0.450661  0.277618 -0.289633  \n",
       "superplasticizer        -0.265999       0.222691 -0.192700  0.366079  \n",
       "coarseaggregate          1.000000      -0.178481 -0.003016 -0.164935  \n",
       "fineaggregate           -0.178481       1.000000 -0.156095 -0.167241  \n",
       "age                     -0.003016      -0.156095  1.000000  0.328873  \n",
       "csMPa                   -0.164935      -0.167241  0.328873  1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Correlation of data\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Correlation Visualisation\n",
    "sns.heatmap(df.corr(),cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y\n",
    "X = df.drop(['csMPa'],axis=1)\n",
    "y = df['csMPa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the Data (MinMax Scaler)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data scaled\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Data scaled\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Models and Layers\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing ANN\n",
    "model = Sequential()\n",
    "\n",
    "#Input and Hidden Layer\n",
    "model.add(Dense(input_dim=8,activation='relu',units=4,kernel_initializer='uniform'))\n",
    "\n",
    "#Second Hidden Layer\n",
    "model.add(Dense(activation='relu',units=4,kernel_initializer='uniform'))\n",
    "\n",
    "#Third Hidden Layer\n",
    "model.add(Dense(activation='relu',units=4,kernel_initializer='uniform'))\n",
    "\n",
    "#Output Layer\n",
    "#No activation function as problem is of regression\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "#loss = 'mse' (regression problem)\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early Stopping Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "12/12 [==============================] - 1s 15ms/step - loss: 1588.3738 - val_loss: 1496.7290\n",
      "Epoch 2/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1586.8564 - val_loss: 1495.2079\n",
      "Epoch 3/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1585.2439 - val_loss: 1493.5796\n",
      "Epoch 4/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1583.4884 - val_loss: 1491.8064\n",
      "Epoch 5/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1581.5702 - val_loss: 1489.8116\n",
      "Epoch 6/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1579.3948 - val_loss: 1487.5665\n",
      "Epoch 7/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1576.9380 - val_loss: 1484.9838\n",
      "Epoch 8/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1574.1184 - val_loss: 1481.9941\n",
      "Epoch 9/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1570.8353 - val_loss: 1478.5205\n",
      "Epoch 10/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1566.9938 - val_loss: 1474.4723\n",
      "Epoch 11/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1562.5164 - val_loss: 1469.6509\n",
      "Epoch 12/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1557.1915 - val_loss: 1463.9941\n",
      "Epoch 13/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1550.9095 - val_loss: 1457.3146\n",
      "Epoch 14/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1543.5574 - val_loss: 1449.4431\n",
      "Epoch 15/600\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1534.8960 - val_loss: 1440.1968\n",
      "Epoch 16/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1524.7347 - val_loss: 1429.5144\n",
      "Epoch 17/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1513.0305 - val_loss: 1417.2637\n",
      "Epoch 18/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1499.5875 - val_loss: 1403.0416\n",
      "Epoch 19/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1483.9717 - val_loss: 1386.7411\n",
      "Epoch 20/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1466.3749 - val_loss: 1368.1913\n",
      "Epoch 21/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1446.1183 - val_loss: 1347.3879\n",
      "Epoch 22/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1423.3933 - val_loss: 1323.7386\n",
      "Epoch 23/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1397.7437 - val_loss: 1297.3093\n",
      "Epoch 24/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1369.3561 - val_loss: 1267.9739\n",
      "Epoch 25/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1337.5519 - val_loss: 1235.8251\n",
      "Epoch 26/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1303.2440 - val_loss: 1200.7065\n",
      "Epoch 27/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1265.9364 - val_loss: 1162.8918\n",
      "Epoch 28/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1225.3904 - val_loss: 1122.1064\n",
      "Epoch 29/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1181.3528 - val_loss: 1078.1547\n",
      "Epoch 30/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1134.8378 - val_loss: 1031.4786\n",
      "Epoch 31/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1085.8207 - val_loss: 982.2497\n",
      "Epoch 32/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1033.4065 - val_loss: 931.9562\n",
      "Epoch 33/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 980.3728 - val_loss: 879.3364\n",
      "Epoch 34/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 925.0394 - val_loss: 824.8762\n",
      "Epoch 35/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 867.8798 - val_loss: 770.4804\n",
      "Epoch 36/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 810.7709 - val_loss: 716.7812\n",
      "Epoch 37/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 754.7125 - val_loss: 663.4656\n",
      "Epoch 38/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 699.0959 - val_loss: 611.3110\n",
      "Epoch 39/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 644.8693 - val_loss: 561.5793\n",
      "Epoch 40/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 593.0838 - val_loss: 513.3880\n",
      "Epoch 41/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 543.7061 - val_loss: 469.3594\n",
      "Epoch 42/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 498.1004 - val_loss: 429.0194\n",
      "Epoch 43/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 455.6438 - val_loss: 392.8462\n",
      "Epoch 44/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 418.2135 - val_loss: 359.7818\n",
      "Epoch 45/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 384.2069 - val_loss: 332.0163\n",
      "Epoch 46/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 355.2576 - val_loss: 309.5059\n",
      "Epoch 47/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 331.9676 - val_loss: 290.8898\n",
      "Epoch 48/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 311.6904 - val_loss: 276.6475\n",
      "Epoch 49/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 296.8337 - val_loss: 264.7826\n",
      "Epoch 50/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 283.4100 - val_loss: 256.2616\n",
      "Epoch 51/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 273.7889 - val_loss: 250.1345\n",
      "Epoch 52/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 266.8617 - val_loss: 245.8260\n",
      "Epoch 53/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 261.4919 - val_loss: 242.9835\n",
      "Epoch 54/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 257.3578 - val_loss: 241.1160\n",
      "Epoch 55/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 254.8519 - val_loss: 239.7637\n",
      "Epoch 56/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 252.3765 - val_loss: 238.9466\n",
      "Epoch 57/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 250.6758 - val_loss: 238.3843\n",
      "Epoch 58/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 249.5731 - val_loss: 238.0145\n",
      "Epoch 59/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 248.5057 - val_loss: 237.6086\n",
      "Epoch 60/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 247.7018 - val_loss: 237.2558\n",
      "Epoch 61/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 246.8506 - val_loss: 236.8734\n",
      "Epoch 62/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 246.1181 - val_loss: 236.4945\n",
      "Epoch 63/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 245.3803 - val_loss: 236.0786\n",
      "Epoch 64/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 244.6282 - val_loss: 235.6143\n",
      "Epoch 65/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 243.9092 - val_loss: 235.1533\n",
      "Epoch 66/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 243.2010 - val_loss: 234.4827\n",
      "Epoch 67/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 242.4962 - val_loss: 233.9364\n",
      "Epoch 68/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 241.8203 - val_loss: 233.4084\n",
      "Epoch 69/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 241.0762 - val_loss: 232.9591\n",
      "Epoch 70/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 240.3669 - val_loss: 232.3526\n",
      "Epoch 71/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 239.6448 - val_loss: 231.8123\n",
      "Epoch 72/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 238.9337 - val_loss: 231.4042\n",
      "Epoch 73/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 238.2201 - val_loss: 230.6645\n",
      "Epoch 74/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 237.5301 - val_loss: 229.9395\n",
      "Epoch 75/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 236.7981 - val_loss: 229.3324\n",
      "Epoch 76/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 236.1171 - val_loss: 228.7652\n",
      "Epoch 77/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 235.4273 - val_loss: 228.2292\n",
      "Epoch 78/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 234.6688 - val_loss: 227.6577\n",
      "Epoch 79/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 233.9430 - val_loss: 226.9540\n",
      "Epoch 80/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 233.2868 - val_loss: 226.3152\n",
      "Epoch 81/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 232.5381 - val_loss: 225.8555\n",
      "Epoch 82/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 231.8131 - val_loss: 225.3505\n",
      "Epoch 83/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 231.1550 - val_loss: 224.9377\n",
      "Epoch 84/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 230.3826 - val_loss: 224.1381\n",
      "Epoch 85/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 229.6425 - val_loss: 223.5610\n",
      "Epoch 86/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 228.9214 - val_loss: 222.9457\n",
      "Epoch 87/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 228.2508 - val_loss: 222.3962\n",
      "Epoch 88/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 227.4998 - val_loss: 221.8697\n",
      "Epoch 89/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 226.8346 - val_loss: 221.3600\n",
      "Epoch 90/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 226.0862 - val_loss: 220.6743\n",
      "Epoch 91/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 225.3838 - val_loss: 220.1321\n",
      "Epoch 92/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 224.7337 - val_loss: 219.4913\n",
      "Epoch 93/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 223.9961 - val_loss: 219.0618\n",
      "Epoch 94/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 223.2700 - val_loss: 218.4383\n",
      "Epoch 95/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 222.5648 - val_loss: 217.9052\n",
      "Epoch 96/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 221.8106 - val_loss: 217.4857\n",
      "Epoch 97/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 221.0604 - val_loss: 217.0071\n",
      "Epoch 98/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 220.3747 - val_loss: 216.5230\n",
      "Epoch 99/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 219.6180 - val_loss: 215.9697\n",
      "Epoch 100/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 219.0172 - val_loss: 215.5458\n",
      "Epoch 101/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 218.1923 - val_loss: 214.7356\n",
      "Epoch 102/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 217.5233 - val_loss: 213.8272\n",
      "Epoch 103/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 216.7605 - val_loss: 213.1419\n",
      "Epoch 104/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 215.9874 - val_loss: 212.7349\n",
      "Epoch 105/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 215.2729 - val_loss: 212.2738\n",
      "Epoch 106/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 214.5508 - val_loss: 211.5493\n",
      "Epoch 107/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 213.8219 - val_loss: 210.9462\n",
      "Epoch 108/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 213.0889 - val_loss: 210.2410\n",
      "Epoch 109/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 212.3572 - val_loss: 209.5855\n",
      "Epoch 110/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 211.6384 - val_loss: 209.0868\n",
      "Epoch 111/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 210.8721 - val_loss: 208.6436\n",
      "Epoch 112/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 210.1341 - val_loss: 208.1182\n",
      "Epoch 113/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 209.4581 - val_loss: 207.3627\n",
      "Epoch 114/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 208.7077 - val_loss: 206.8189\n",
      "Epoch 115/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 207.9341 - val_loss: 206.2273\n",
      "Epoch 116/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 207.2150 - val_loss: 205.4614\n",
      "Epoch 117/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 206.4818 - val_loss: 204.8612\n",
      "Epoch 118/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 205.7539 - val_loss: 204.1348\n",
      "Epoch 119/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 204.9598 - val_loss: 203.5000\n",
      "Epoch 120/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 204.2662 - val_loss: 203.0043\n",
      "Epoch 121/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 203.4996 - val_loss: 202.3403\n",
      "Epoch 122/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 202.7864 - val_loss: 201.7601\n",
      "Epoch 123/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 202.0608 - val_loss: 201.1004\n",
      "Epoch 124/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 201.3825 - val_loss: 200.3419\n",
      "Epoch 125/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 200.6046 - val_loss: 199.8864\n",
      "Epoch 126/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 199.8789 - val_loss: 199.2861\n",
      "Epoch 127/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 199.1669 - val_loss: 198.5764\n",
      "Epoch 128/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 198.3967 - val_loss: 198.0505\n",
      "Epoch 129/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 197.6847 - val_loss: 197.5833\n",
      "Epoch 130/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 196.9677 - val_loss: 197.0624\n",
      "Epoch 131/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 196.1992 - val_loss: 196.4756\n",
      "Epoch 132/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 195.4660 - val_loss: 195.8013\n",
      "Epoch 133/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 194.7299 - val_loss: 195.0959\n",
      "Epoch 134/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 194.1614 - val_loss: 194.2875\n",
      "Epoch 135/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 193.2732 - val_loss: 193.8307\n",
      "Epoch 136/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 192.5202 - val_loss: 193.2213\n",
      "Epoch 137/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 191.7858 - val_loss: 192.6141\n",
      "Epoch 138/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 191.0382 - val_loss: 191.9274\n",
      "Epoch 139/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 190.3140 - val_loss: 191.3658\n",
      "Epoch 140/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 189.6206 - val_loss: 190.9116\n",
      "Epoch 141/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 188.8342 - val_loss: 190.4346\n",
      "Epoch 142/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 188.0788 - val_loss: 189.5983\n",
      "Epoch 143/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 187.2835 - val_loss: 189.0024\n",
      "Epoch 144/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 186.5983 - val_loss: 188.2623\n",
      "Epoch 145/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 185.8292 - val_loss: 187.7800\n",
      "Epoch 146/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 185.1313 - val_loss: 187.1187\n",
      "Epoch 147/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 184.3341 - val_loss: 186.5463\n",
      "Epoch 148/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 183.8333 - val_loss: 186.2026\n",
      "Epoch 149/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 182.8990 - val_loss: 185.2900\n",
      "Epoch 150/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 182.1232 - val_loss: 184.5180\n",
      "Epoch 151/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 181.3738 - val_loss: 183.8830\n",
      "Epoch 152/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 180.6363 - val_loss: 183.3268\n",
      "Epoch 153/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 179.9474 - val_loss: 182.9433\n",
      "Epoch 154/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 179.1243 - val_loss: 182.1236\n",
      "Epoch 155/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 178.3376 - val_loss: 181.4669\n",
      "Epoch 156/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 177.5853 - val_loss: 180.9557\n",
      "Epoch 157/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 176.9299 - val_loss: 180.5247\n",
      "Epoch 158/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 176.0870 - val_loss: 179.7776\n",
      "Epoch 159/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 175.2853 - val_loss: 179.0403\n",
      "Epoch 160/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 174.6049 - val_loss: 178.1983\n",
      "Epoch 161/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 173.9373 - val_loss: 177.7807\n",
      "Epoch 162/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 173.1350 - val_loss: 177.0137\n",
      "Epoch 163/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 172.5075 - val_loss: 176.2368\n",
      "Epoch 164/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 171.6989 - val_loss: 175.6567\n",
      "Epoch 165/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 170.9815 - val_loss: 175.1912\n",
      "Epoch 166/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 170.2388 - val_loss: 174.6135\n",
      "Epoch 167/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 169.5141 - val_loss: 174.0056\n",
      "Epoch 168/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 168.8294 - val_loss: 173.3100\n",
      "Epoch 169/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 168.1207 - val_loss: 172.6432\n",
      "Epoch 170/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 167.3656 - val_loss: 171.9229\n",
      "Epoch 171/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 166.6404 - val_loss: 171.3817\n",
      "Epoch 172/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 165.9273 - val_loss: 170.6688\n",
      "Epoch 173/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 165.2302 - val_loss: 170.1933\n",
      "Epoch 174/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 164.5036 - val_loss: 169.5230\n",
      "Epoch 175/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 163.8023 - val_loss: 169.0148\n",
      "Epoch 176/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 163.0573 - val_loss: 168.4250\n",
      "Epoch 177/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 162.3749 - val_loss: 167.9151\n",
      "Epoch 178/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 161.6998 - val_loss: 167.3569\n",
      "Epoch 179/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 160.9675 - val_loss: 166.6776\n",
      "Epoch 180/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 160.2904 - val_loss: 165.9217\n",
      "Epoch 181/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 159.5749 - val_loss: 165.3829\n",
      "Epoch 182/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 158.8758 - val_loss: 164.8912\n",
      "Epoch 183/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 158.1674 - val_loss: 164.1937\n",
      "Epoch 184/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 157.4726 - val_loss: 163.5555\n",
      "Epoch 185/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 156.7879 - val_loss: 163.0676\n",
      "Epoch 186/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 156.0943 - val_loss: 162.5636\n",
      "Epoch 187/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 155.4343 - val_loss: 161.9073\n",
      "Epoch 188/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 154.7993 - val_loss: 161.4151\n",
      "Epoch 189/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 154.0793 - val_loss: 160.6825\n",
      "Epoch 190/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 153.4703 - val_loss: 160.0918\n",
      "Epoch 191/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 152.8357 - val_loss: 159.5662\n",
      "Epoch 192/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 152.1696 - val_loss: 158.8890\n",
      "Epoch 193/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 151.5805 - val_loss: 158.3356\n",
      "Epoch 194/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 150.9362 - val_loss: 157.8421\n",
      "Epoch 195/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 150.3828 - val_loss: 157.5906\n",
      "Epoch 196/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 149.6235 - val_loss: 157.0637\n",
      "Epoch 197/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 148.9526 - val_loss: 156.5390\n",
      "Epoch 198/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 148.3625 - val_loss: 156.1751\n",
      "Epoch 199/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 147.7332 - val_loss: 155.6446\n",
      "Epoch 200/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 147.0445 - val_loss: 155.0031\n",
      "Epoch 201/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 146.4639 - val_loss: 154.1977\n",
      "Epoch 202/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 145.7741 - val_loss: 153.5347\n",
      "Epoch 203/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 145.2297 - val_loss: 152.9271\n",
      "Epoch 204/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 144.4930 - val_loss: 152.5730\n",
      "Epoch 205/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 144.0061 - val_loss: 152.1587\n",
      "Epoch 206/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 143.3278 - val_loss: 151.5115\n",
      "Epoch 207/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 142.7273 - val_loss: 150.9062\n",
      "Epoch 208/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 142.1568 - val_loss: 150.3750\n",
      "Epoch 209/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 141.5540 - val_loss: 149.8839\n",
      "Epoch 210/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 141.1022 - val_loss: 149.6875\n",
      "Epoch 211/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 140.4509 - val_loss: 149.0789\n",
      "Epoch 212/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 139.8938 - val_loss: 148.4032\n",
      "Epoch 213/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 139.3203 - val_loss: 147.8547\n",
      "Epoch 214/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 138.7765 - val_loss: 147.3930\n",
      "Epoch 215/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 138.2812 - val_loss: 146.9108\n",
      "Epoch 216/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 137.7924 - val_loss: 146.3698\n",
      "Epoch 217/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 137.3017 - val_loss: 145.9334\n",
      "Epoch 218/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 136.8207 - val_loss: 145.4916\n",
      "Epoch 219/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 136.3292 - val_loss: 145.0678\n",
      "Epoch 220/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 135.8199 - val_loss: 144.6298\n",
      "Epoch 221/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 135.3842 - val_loss: 144.1159\n",
      "Epoch 222/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 134.9136 - val_loss: 143.7355\n",
      "Epoch 223/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 134.4987 - val_loss: 143.3316\n",
      "Epoch 224/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 133.9574 - val_loss: 143.0566\n",
      "Epoch 225/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 133.5050 - val_loss: 142.6572\n",
      "Epoch 226/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 133.0979 - val_loss: 142.1814\n",
      "Epoch 227/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 132.7239 - val_loss: 141.9635\n",
      "Epoch 228/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 132.2377 - val_loss: 141.5593\n",
      "Epoch 229/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 131.8748 - val_loss: 140.9505\n",
      "Epoch 230/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 131.4025 - val_loss: 140.4348\n",
      "Epoch 231/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 131.0069 - val_loss: 140.0592\n",
      "Epoch 232/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 130.6686 - val_loss: 139.4952\n",
      "Epoch 233/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 130.2266 - val_loss: 139.1303\n",
      "Epoch 234/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 129.8387 - val_loss: 138.8506\n",
      "Epoch 235/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 129.4296 - val_loss: 138.5404\n",
      "Epoch 236/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 129.1568 - val_loss: 138.1104\n",
      "Epoch 237/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 128.7078 - val_loss: 137.9370\n",
      "Epoch 238/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 128.3618 - val_loss: 137.6063\n",
      "Epoch 239/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 128.0258 - val_loss: 137.2025\n",
      "Epoch 240/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 127.7947 - val_loss: 136.7112\n",
      "Epoch 241/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 127.2969 - val_loss: 136.4158\n",
      "Epoch 242/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 126.9672 - val_loss: 136.1611\n",
      "Epoch 243/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 126.7076 - val_loss: 135.7303\n",
      "Epoch 244/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 126.3272 - val_loss: 135.4697\n",
      "Epoch 245/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 126.1074 - val_loss: 135.2677\n",
      "Epoch 246/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 125.7292 - val_loss: 134.9117\n",
      "Epoch 247/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 125.3898 - val_loss: 134.6899\n",
      "Epoch 248/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 125.1925 - val_loss: 134.3391\n",
      "Epoch 249/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.8797 - val_loss: 134.0981\n",
      "Epoch 250/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.6211 - val_loss: 133.7658\n",
      "Epoch 251/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.4373 - val_loss: 133.6028\n",
      "Epoch 252/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 124.1293 - val_loss: 133.1527\n",
      "Epoch 253/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 123.8849 - val_loss: 132.8324\n",
      "Epoch 254/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 123.5989 - val_loss: 132.6296\n",
      "Epoch 255/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 123.3391 - val_loss: 132.5318\n",
      "Epoch 256/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 123.2153 - val_loss: 132.3188\n",
      "Epoch 257/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 122.9615 - val_loss: 131.8947\n",
      "Epoch 258/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 122.7414 - val_loss: 131.4885\n",
      "Epoch 259/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 122.5578 - val_loss: 131.2791\n",
      "Epoch 260/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 122.3178 - val_loss: 131.0608\n",
      "Epoch 261/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 122.0760 - val_loss: 130.8807\n",
      "Epoch 262/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.9646 - val_loss: 130.8316\n",
      "Epoch 263/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.7220 - val_loss: 130.4299\n",
      "Epoch 264/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 121.5826 - val_loss: 130.1866\n",
      "Epoch 265/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.3881 - val_loss: 129.9892\n",
      "Epoch 266/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.2328 - val_loss: 129.7682\n",
      "Epoch 267/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 121.0323 - val_loss: 129.5096\n",
      "Epoch 268/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 120.9153 - val_loss: 129.2572\n",
      "Epoch 269/600\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 120.7251 - val_loss: 129.0482\n",
      "Epoch 270/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 120.7451 - val_loss: 129.0119\n",
      "Epoch 271/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 120.4301 - val_loss: 128.7718\n",
      "Epoch 272/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 120.2656 - val_loss: 128.5893\n",
      "Epoch 273/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 120.1221 - val_loss: 128.3996\n",
      "Epoch 274/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 120.0349 - val_loss: 128.2952\n",
      "Epoch 275/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.8289 - val_loss: 127.9601\n",
      "Epoch 276/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.7298 - val_loss: 127.7303\n",
      "Epoch 277/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 119.7118 - val_loss: 127.6103\n",
      "Epoch 278/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.5303 - val_loss: 127.5438\n",
      "Epoch 279/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.3444 - val_loss: 127.5034\n",
      "Epoch 280/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.2328 - val_loss: 127.4062\n",
      "Epoch 281/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 119.1231 - val_loss: 127.1744\n",
      "Epoch 282/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.9919 - val_loss: 126.9316\n",
      "Epoch 283/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.9100 - val_loss: 126.7400\n",
      "Epoch 284/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.9332 - val_loss: 126.6394\n",
      "Epoch 285/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.7903 - val_loss: 126.4922\n",
      "Epoch 286/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.6108 - val_loss: 126.3743\n",
      "Epoch 287/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.5687 - val_loss: 126.3127\n",
      "Epoch 288/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.4447 - val_loss: 126.1755\n",
      "Epoch 289/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.4198 - val_loss: 126.1124\n",
      "Epoch 290/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.3149 - val_loss: 125.8536\n",
      "Epoch 291/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.1911 - val_loss: 125.9033\n",
      "Epoch 292/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.1258 - val_loss: 125.8545\n",
      "Epoch 293/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 118.0726 - val_loss: 125.6382\n",
      "Epoch 294/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.9774 - val_loss: 125.3343\n",
      "Epoch 295/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.8651 - val_loss: 125.1608\n",
      "Epoch 296/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.7701 - val_loss: 125.0190\n",
      "Epoch 297/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.6769 - val_loss: 124.8747\n",
      "Epoch 298/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.6224 - val_loss: 124.6751\n",
      "Epoch 299/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.5706 - val_loss: 124.6038\n",
      "Epoch 300/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.4491 - val_loss: 124.4449\n",
      "Epoch 301/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.6322 - val_loss: 124.3094\n",
      "Epoch 302/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.3422 - val_loss: 124.2433\n",
      "Epoch 303/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.3107 - val_loss: 124.0898\n",
      "Epoch 304/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.2088 - val_loss: 123.9942\n",
      "Epoch 305/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.1614 - val_loss: 123.8966\n",
      "Epoch 306/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.0807 - val_loss: 123.7817\n",
      "Epoch 307/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 117.0339 - val_loss: 123.7134\n",
      "Epoch 308/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 117.1370 - val_loss: 123.6095\n",
      "Epoch 309/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.9199 - val_loss: 123.5226\n",
      "Epoch 310/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.8725 - val_loss: 123.4449\n",
      "Epoch 311/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.8309 - val_loss: 123.4475\n",
      "Epoch 312/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.8534 - val_loss: 123.5493\n",
      "Epoch 313/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 116.7303 - val_loss: 123.3629\n",
      "Epoch 314/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.6807 - val_loss: 123.1884\n",
      "Epoch 315/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.6347 - val_loss: 123.0608\n",
      "Epoch 316/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 116.5682 - val_loss: 122.8920\n",
      "Epoch 317/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.4866 - val_loss: 122.8205\n",
      "Epoch 318/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.4122 - val_loss: 122.7548\n",
      "Epoch 319/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.4183 - val_loss: 122.7791\n",
      "Epoch 320/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.3746 - val_loss: 122.7468\n",
      "Epoch 321/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.2710 - val_loss: 122.5846\n",
      "Epoch 322/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.2728 - val_loss: 122.3779\n",
      "Epoch 323/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 116.2223 - val_loss: 122.3990\n",
      "Epoch 324/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.1418 - val_loss: 122.2731\n",
      "Epoch 325/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.1222 - val_loss: 122.3570\n",
      "Epoch 326/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.0368 - val_loss: 122.2067\n",
      "Epoch 327/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 116.0009 - val_loss: 122.2393\n",
      "Epoch 328/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.9453 - val_loss: 122.1196\n",
      "Epoch 329/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.8749 - val_loss: 121.9222\n",
      "Epoch 330/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.8285 - val_loss: 121.8072\n",
      "Epoch 331/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.7357 - val_loss: 121.7904\n",
      "Epoch 332/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.7327 - val_loss: 121.7272\n",
      "Epoch 333/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.7546 - val_loss: 121.7967\n",
      "Epoch 334/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.7691 - val_loss: 121.7463\n",
      "Epoch 335/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.6585 - val_loss: 121.4273\n",
      "Epoch 336/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.6164 - val_loss: 121.4617\n",
      "Epoch 337/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.5431 - val_loss: 121.3933\n",
      "Epoch 338/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.5117 - val_loss: 121.1783\n",
      "Epoch 339/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.4776 - val_loss: 121.0301\n",
      "Epoch 340/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.3651 - val_loss: 121.0385\n",
      "Epoch 341/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.3317 - val_loss: 120.9633\n",
      "Epoch 342/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.2780 - val_loss: 120.8866\n",
      "Epoch 343/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.2590 - val_loss: 120.7868\n",
      "Epoch 344/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 115.2050 - val_loss: 120.5999\n",
      "Epoch 345/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.2543 - val_loss: 120.5519\n",
      "Epoch 346/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.2274 - val_loss: 120.4927\n",
      "Epoch 347/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.0899 - val_loss: 120.4912\n",
      "Epoch 348/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.1265 - val_loss: 120.5026\n",
      "Epoch 349/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 115.1290 - val_loss: 120.3559\n",
      "Epoch 350/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.9546 - val_loss: 120.4695\n",
      "Epoch 351/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.9962 - val_loss: 120.3991\n",
      "Epoch 352/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.9333 - val_loss: 120.3033\n",
      "Epoch 353/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.8720 - val_loss: 120.1367\n",
      "Epoch 354/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.8768 - val_loss: 120.0802\n",
      "Epoch 355/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.8227 - val_loss: 119.9948\n",
      "Epoch 356/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.7702 - val_loss: 119.9500\n",
      "Epoch 357/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.7225 - val_loss: 119.9338\n",
      "Epoch 358/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.7359 - val_loss: 119.8440\n",
      "Epoch 359/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.6804 - val_loss: 119.7655\n",
      "Epoch 360/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.7585 - val_loss: 119.8946\n",
      "Epoch 361/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.5673 - val_loss: 119.5937\n",
      "Epoch 362/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.5756 - val_loss: 119.4851\n",
      "Epoch 363/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.5684 - val_loss: 119.5107\n",
      "Epoch 364/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.4864 - val_loss: 119.4345\n",
      "Epoch 365/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.5266 - val_loss: 119.3942\n",
      "Epoch 366/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.4476 - val_loss: 119.3736\n",
      "Epoch 367/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.4535 - val_loss: 119.2245\n",
      "Epoch 368/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.3744 - val_loss: 119.1609\n",
      "Epoch 369/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.3818 - val_loss: 119.0781\n",
      "Epoch 370/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.3133 - val_loss: 119.1158\n",
      "Epoch 371/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 114.3039 - val_loss: 119.0984\n",
      "Epoch 372/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.2534 - val_loss: 118.9327\n",
      "Epoch 373/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.2301 - val_loss: 118.8801\n",
      "Epoch 374/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.2703 - val_loss: 118.8433\n",
      "Epoch 375/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.1941 - val_loss: 118.7528\n",
      "Epoch 376/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.3015 - val_loss: 118.6622\n",
      "Epoch 377/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 114.1173 - val_loss: 118.7041\n",
      "Epoch 378/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.0529 - val_loss: 118.6594\n",
      "Epoch 379/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.0360 - val_loss: 118.5858\n",
      "Epoch 380/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.0432 - val_loss: 118.4761\n",
      "Epoch 381/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 114.1110 - val_loss: 118.4018\n",
      "Epoch 382/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.9229 - val_loss: 118.3970\n",
      "Epoch 383/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.9051 - val_loss: 118.4785\n",
      "Epoch 384/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 113.9333 - val_loss: 118.3791\n",
      "Epoch 385/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.9435 - val_loss: 118.2017\n",
      "Epoch 386/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.9061 - val_loss: 118.2960\n",
      "Epoch 387/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.8218 - val_loss: 118.1478\n",
      "Epoch 388/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 113.7275 - val_loss: 117.9809\n",
      "Epoch 389/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 113.8733 - val_loss: 117.9484\n",
      "Epoch 390/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.8600 - val_loss: 117.9800\n",
      "Epoch 391/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 113.6298 - val_loss: 117.8832\n",
      "Epoch 392/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 113.7448 - val_loss: 117.8138\n",
      "Epoch 393/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 113.6028 - val_loss: 117.7857\n",
      "Epoch 394/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.7200 - val_loss: 117.8743\n",
      "Epoch 395/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.5459 - val_loss: 117.7247\n",
      "Epoch 396/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.6528 - val_loss: 117.7503\n",
      "Epoch 397/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.5672 - val_loss: 117.7310\n",
      "Epoch 398/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.5223 - val_loss: 117.7845\n",
      "Epoch 399/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.5157 - val_loss: 117.5969\n",
      "Epoch 400/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.4674 - val_loss: 117.6031\n",
      "Epoch 401/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.4146 - val_loss: 117.5230\n",
      "Epoch 402/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.4026 - val_loss: 117.4559\n",
      "Epoch 403/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 113.4847 - val_loss: 117.3995\n",
      "Epoch 404/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.4720 - val_loss: 117.3448\n",
      "Epoch 405/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 113.3549 - val_loss: 117.3849\n",
      "Epoch 406/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 113.2841 - val_loss: 117.3141\n",
      "Epoch 407/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.2488 - val_loss: 117.2746\n",
      "Epoch 408/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 113.2225 - val_loss: 117.2095\n",
      "Epoch 409/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.2622 - val_loss: 117.1729\n",
      "Epoch 410/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.2020 - val_loss: 117.2113\n",
      "Epoch 411/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.3017 - val_loss: 117.0901\n",
      "Epoch 412/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.1951 - val_loss: 117.1570\n",
      "Epoch 413/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.1843 - val_loss: 117.0844\n",
      "Epoch 414/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 113.1847 - val_loss: 117.0911\n",
      "Epoch 415/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 113.1139 - val_loss: 116.9922\n",
      "Epoch 416/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 113.0395 - val_loss: 116.9741\n",
      "Epoch 417/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 113.0229 - val_loss: 116.9818\n",
      "Epoch 418/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 113.0218 - val_loss: 116.9734\n",
      "Epoch 419/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.9648 - val_loss: 116.8625\n",
      "Epoch 420/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 112.9399 - val_loss: 116.7499\n",
      "Epoch 421/600\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 112.9317 - val_loss: 116.6353\n",
      "Epoch 422/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 112.9280 - val_loss: 116.6434\n",
      "Epoch 423/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.8553 - val_loss: 116.6538\n",
      "Epoch 424/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 113.1227 - val_loss: 116.6727\n",
      "Epoch 425/600\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 112.8510 - val_loss: 116.4550\n",
      "Epoch 426/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.8490 - val_loss: 116.4513\n",
      "Epoch 427/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.8085 - val_loss: 116.4430\n",
      "Epoch 428/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.7321 - val_loss: 116.4730\n",
      "Epoch 429/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 112.7220 - val_loss: 116.4693\n",
      "Epoch 430/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.7407 - val_loss: 116.4538\n",
      "Epoch 431/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.7358 - val_loss: 116.3336\n",
      "Epoch 432/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.6738 - val_loss: 116.3625\n",
      "Epoch 433/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.6312 - val_loss: 116.3531\n",
      "Epoch 434/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 112.8094 - val_loss: 116.4939\n",
      "Epoch 435/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.5946 - val_loss: 116.3271\n",
      "Epoch 436/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.6635 - val_loss: 116.2254\n",
      "Epoch 437/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 112.5803 - val_loss: 116.1724\n",
      "Epoch 438/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.5563 - val_loss: 116.1438\n",
      "Epoch 439/600\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 112.5951 - val_loss: 116.1140\n",
      "Epoch 440/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.4884 - val_loss: 116.1435\n",
      "Epoch 441/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.5038 - val_loss: 116.1268\n",
      "Epoch 442/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.4490 - val_loss: 115.9839\n",
      "Epoch 443/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.5407 - val_loss: 115.9053\n",
      "Epoch 444/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.4464 - val_loss: 115.9305\n",
      "Epoch 445/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.4183 - val_loss: 115.8557\n",
      "Epoch 446/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.3626 - val_loss: 115.7971\n",
      "Epoch 447/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.3834 - val_loss: 115.7365\n",
      "Epoch 448/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.3568 - val_loss: 115.7473\n",
      "Epoch 449/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 112.3026 - val_loss: 115.6840\n",
      "Epoch 450/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 112.2880 - val_loss: 115.6361\n",
      "Epoch 451/600\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 112.3217 - val_loss: 115.7114\n",
      "Epoch 452/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 112.2767 - val_loss: 115.6077\n",
      "Epoch 453/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.2585 - val_loss: 115.5903\n",
      "Epoch 454/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.2059 - val_loss: 115.6651\n",
      "Epoch 455/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 112.1867 - val_loss: 115.6017\n",
      "Epoch 456/600\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 112.1377 - val_loss: 115.5856\n",
      "Epoch 457/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.1402 - val_loss: 115.5388\n",
      "Epoch 458/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.1842 - val_loss: 115.4656\n",
      "Epoch 459/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.0606 - val_loss: 115.4964\n",
      "Epoch 460/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.0969 - val_loss: 115.4226\n",
      "Epoch 461/600\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 112.0781 - val_loss: 115.4932\n",
      "Epoch 462/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 112.0778 - val_loss: 115.5386\n",
      "Epoch 463/600\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 112.0905 - val_loss: 115.4929\n",
      "Epoch 464/600\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 112.0246 - val_loss: 115.4889\n",
      "Epoch 465/600\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 111.9992 - val_loss: 115.4345\n",
      "Epoch 00465: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2535cde1580>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled,y_train,\n",
    "          validation_data=(X_test_scaled,y_test),\n",
    "         batch_size=64,epochs=600,\n",
    "         callbacks=[early_stop])\n",
    "\n",
    "#This validation test is just for knowing how is model performing after training each epoch\n",
    "#It is no where related to model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that final no. of epochs is 465 where loss is not minimizing now !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1588.373779</td>\n",
       "      <td>1496.729004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1586.856445</td>\n",
       "      <td>1495.207886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1585.243896</td>\n",
       "      <td>1493.579590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1583.488403</td>\n",
       "      <td>1491.806396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1581.570190</td>\n",
       "      <td>1489.811646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>112.078056</td>\n",
       "      <td>115.493172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>112.077789</td>\n",
       "      <td>115.538559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>112.090500</td>\n",
       "      <td>115.492859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>112.024551</td>\n",
       "      <td>115.488876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>111.999184</td>\n",
       "      <td>115.434540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss     val_loss\n",
       "0    1588.373779  1496.729004\n",
       "1    1586.856445  1495.207886\n",
       "2    1585.243896  1493.579590\n",
       "3    1583.488403  1491.806396\n",
       "4    1581.570190  1489.811646\n",
       "..           ...          ...\n",
       "460   112.078056   115.493172\n",
       "461   112.077789   115.538559\n",
       "462   112.090500   115.492859\n",
       "463   112.024551   115.488876\n",
       "464   111.999184   115.434540\n",
       "\n",
       "[465 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training Loss and Validation Loss\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()\n",
    "\n",
    "#Perfect behaviour where the training loss is also decreasing with validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "1. Mean Absolute Error\n",
    "2. Mean Squared Error\n",
    "3. Explained Variance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making Predictions\n",
    "predictions = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.590451662995669"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean Absolute error\n",
    "mean_absolute_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.43453720117847"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mean Squared Error\n",
    "mean_squared_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1030.000000\n",
       "mean       35.817961\n",
       "std        16.705742\n",
       "min         2.330000\n",
       "25%        23.710000\n",
       "50%        34.445000\n",
       "75%        46.135000\n",
       "max        82.600000\n",
       "Name: csMPa, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['csMPa'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Average is around 36% whereas mean absolute error is 8% which is around 22% of average value. The model is not so great but not worst also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5740247663638693"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explained Variance score\n",
    "explained_variance_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 0s/step - loss: 115.4345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115.43453979492188"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Concrete Strength Predictor Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = load_model('Concrete Strength Predictor Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 115.4345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "115.43453979492188"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>389.9</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>145.9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>362.6</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>164.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>944.7</td>\n",
       "      <td>755.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>824.0</td>\n",
       "      <td>869.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
       "31    266.0  114.0     0.0  228.0               0.0            932.0   \n",
       "109   362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "136   389.9  189.0     0.0  145.9              22.0            944.7   \n",
       "88    362.6  189.0     0.0  164.9              11.6            944.7   \n",
       "918   145.0    0.0   179.0  202.0               8.0            824.0   \n",
       "\n",
       "     fineaggregate  age  \n",
       "31           670.0  365  \n",
       "109          755.8    7  \n",
       "136          755.8   28  \n",
       "88           755.8    3  \n",
       "918          869.0   28  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking it as new data\n",
    "new_data = X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement              266.0\n",
       "slag                114.0\n",
       "flyash                0.0\n",
       "water               228.0\n",
       "superplasticizer      0.0\n",
       "coarseaggregate     932.0\n",
       "fineaggregate       670.0\n",
       "age                 365.0\n",
       "Name: 31, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_scaled = scaler.transform([new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60.66418]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.predict(new_data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for this new data the concrete strength is around 60.67"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
